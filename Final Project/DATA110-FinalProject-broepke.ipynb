{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ddd0fa9",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "**DATA110: Intro to Machine Learning**  \n",
    "Brian Roepke  \n",
    "April 24th, 2021  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75c34e",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "analysis objective and the ML approach applied as well as why you selected it.\n",
    "\n",
    "\n",
    "Yelp has opened up a large portion of its dataset for academic and personal use. It contains a massive amount of textual data from their reviews and other information about their businesses. I propose to build an NLP model building a sentiment analysis and mapping the polarity to the rating given for the review. Additionally, a machine learning model will predict the highest-rated business by location, the number of reviews, and categories assigned. Finally, I will attempt to use TensorFlow to create a Neural Network. While we only loosely covered this in class, I will try to combine the Machine Learning methods we learned with the tutorials from class to build a classification model off of the text\n",
    "\n",
    "![Yelp](https://raw.githubusercontent.com/broepke/DATA110/main/Final%20Project/yelp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9abef8",
   "metadata": {},
   "source": [
    "# EDA/DATA PREP\n",
    "\n",
    "explore data issues that may require cleansing, data wrangling/munging, etc..  Include visualizations, statistical analysis, etc.. to better understand the data such as how it's distributed and correlations. You must include analysis/text that explains the meaning of each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "import warnings\n",
    "from timeit import timeit\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# NLTK Imports and Downloads\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9096849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a699ec",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c589cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cb1b3",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "\n",
    "Nulll values are generally not desireable in a dataset.  In certain cases, observations (rows) with low counts will simply be dropped, in other cases, they can be filled with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62784d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NULL values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4858f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_open'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a64d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['is_open'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343145aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop is_open column\n",
    "# df.drop(columns=['is_open'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45846c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the long and lat columns.  Precise location isn't needed\n",
    "df.drop(columns=['longitude', 'latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae60e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column 'text_len' that counts the length for the derived field\n",
    "df['text_len'] = df.apply(lambda row: len(row['text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] =  pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142eab5",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "A common practice is to review any duplicates.  If there are large quantities, they can skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156452a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "len_after = df.shape[0]\n",
    "\n",
    "print(\"Before =\", len_before)\n",
    "# drop duplicates\n",
    "print(\"After =\", len_after)\n",
    "print('')\n",
    "print(\"Total Removed =\", len_before - len_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abcdad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show counts values of each categorical variable\n",
    "print (df['city'].value_counts(), '\\n')\n",
    "print (df['state'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ca71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df = df[df['state'] != 'KS'].copy()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4a38f",
   "metadata": {},
   "source": [
    "**Note:** There are not enough observations for Kansas, these have been dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1e3c0",
   "metadata": {},
   "source": [
    "**Variable Descriptions:**  \n",
    "\n",
    "1. **business_id:** string, 22 character unique string business id\n",
    "1. **name:** string, the business's name\n",
    "1. **address:** string, the full address of the business\n",
    "1. **city:** string, the city\n",
    "1. **state:** string, 2 character state code, if applicable\n",
    "1. **postal code:** string, the postal code\n",
    "1. **latitude:** float, latitude\n",
    "1. **longitude:** float, longitude\n",
    "1. **stars_x:** float, star rating, rounded to half-stars\n",
    "1. **review_count:** integer, number of reviews\n",
    "1. **is_open:** integer, 0 or 1 for closed or open, respectively\n",
    "1. **attributes:** object, business attributes to values. note: some attribute values might be objects\n",
    "1. **categories:** an array of strings of business categories\n",
    "1. **hours:** an object of key day to value hours, hours are using a 24hr clock\n",
    "1. **review_id:** string, 22 character unique string review id\n",
    "1. **stars_y:** integer, star rating (the data is a float64 here, but only valid numbers were integers 1-5)\n",
    "1. **useful:** integer, number of useful votes received\n",
    "1. **funny:** integer, number of funny votes received\n",
    "1. **cool:** integer, number of cool votes received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for positive or negative\n",
    "df['target'] = df['stars_x'].apply(lambda c: 0 if c < 4 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d709b",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18069fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(corr, cmap=\"Blues\", annot=True, square=False, ax=ax,  linewidth = 1)\n",
    "plt.title('Pearson Correlation of Features')\n",
    "plt.yticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36539a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x='state', palette='tab20c')\n",
    "plt.title(\"Total Count of Reviews by State\", weight='bold').set_fontsize('16')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x=\"stars_x\", palette='tab20c')\n",
    "plt.title(\"Total Count of Reviews by Star Value\", weight='bold').set_fontsize('16')\n",
    "plt.xlabel(\"Star Value\")\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = df.select_dtypes(include=['int64'])\n",
    "numeric_data.plot(figsize=(15,10), kind='density',subplots=True, layout=(4,3), sharex=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(x='review_count', data=df, bins=50);\n",
    "plt.title(\"Distribution of Count of Reviews\", weight='bold').set_fontsize('16')\n",
    "plt.xlabel(\"Review Count\")\n",
    "plt.ylabel(\"Bin Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=df, x='state', y='stars_x', palette=\"tab20c\")\n",
    "plt.title(\"Star Disribution by State\", weight='bold').set_fontsize('16')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Stars\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bf201",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(data=df, x='is_open', y='stars_x', palette=\"tab20c\")\n",
    "plt.title(\"Star Disribution by Open/Closed\", weight='bold').set_fontsize('16')\n",
    "plt.xlabel(\"Open = 1 / Closed = 0\")\n",
    "plt.ylabel(\"Stars\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910a08a-489b-49c5-9c98-0e66024fdbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_reviews = df.groupby('name').mean().sort_values('review_count', ascending=False)[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf74fa-c606-4673-b4c3-dc90af5dcd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_reviews['review_count'].plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff658a6a-9b6c-4699-aa3e-a6ad4cd2b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = df.groupby(pd.Grouper(key='date',freq='Y')).sum()\n",
    "df_year['review_count'].plot(kind='bar', title=\"Sum of Reviews by Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407e4f7",
   "metadata": {},
   "source": [
    "### Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the number of outliers in a column.\n",
    "def get_outliers(df):\n",
    "    '''Function to identify the number of outliers +/- 3 standard deviations outside of mean.\n",
    "    Pass this function a dataframe and it returns a dictionary'''\n",
    "    \n",
    "    outs = {}\n",
    "    \n",
    "    df = df.select_dtypes(include=['int64'])\n",
    "\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        # calculate summary statistics\n",
    "        data_mean, data_std = np.mean(df[col]), np.std(df[col])\n",
    "        \n",
    "        # identify outliers\n",
    "        cut_off = data_std * 3\n",
    "        lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "        \n",
    "        # identify outliers\n",
    "        outliers = [x for x in df[col] if x < lower or x > upper]\n",
    "        \n",
    "        outs[col] = len(outliers)\n",
    "        \n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd108e55",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "For **Parts** of our analysis, the text needs to have some basic transformation for our models to work propertly.  These are as follows:\n",
    "\n",
    "1. **Lower**: Convert all characters to lowercase\n",
    "1. **Remove Punctuation**: In most cases, punctuation doesn't help NLP and ML models and can be removed.\n",
    "1. **Stop Word Removal**: Stop words generally don't add context to analysis (unless the length of text is very short (`100` - `200` characters) and can be removed.\n",
    "1. **Lemmatization**: Words will be reduced to there *Lemma* or root.  This will greatly improve the accuracy of the analysis since words like `simming` and `swimmer` will be reduced to `swim`.\n",
    "\n",
    "**Note**: The orginal text will be preserved for other analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text, stem=\"None\"):\n",
    "    \n",
    "    final_string = \"\"\n",
    "    \n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "    useless_words = useless_words + ['.', ',', '!', \"'\"]\n",
    "    \n",
    "    # Remove stop words\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "    \n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub('\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "    \n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "    \n",
    "    for word in text_stemmed:\n",
    "        final_string += word + \" \"\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x: clean_string(x, stem='Lem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7986a58",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "For our sentiment analysis section, we will be using the `TextBlob` package to assist in creating `polarity scores` or sentiment scores that range from `-1` to `1` where lower scores are more negative and higher more positive.  Based off of these scores, we'll add a classifier of `1` for positive and `0` for negative to be used later in our prediction model. \n",
    "\n",
    "**Note**: `0` is technically nuetral sentiment, we'll verify how many observations were neutral before assuming we can use a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(x):\n",
    "    '''using TextBlob, get the sentiment score for a given body of text'''\n",
    "    blob = TextBlob(x)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Polarity Scoring from TextBlob\n",
    "df['sentiment'] = df['text'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few extra columns to aid in the analysis\n",
    "df['sentiment_label'] = df['sentiment'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[-3:]].sample(5, random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df, x='sentiment', palette=\"tab20c\", bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f6f28",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "The distributions of sentiment, similar to the `1-5` star reviews is left skewed to the positive.  There are very few that have a `<0` polartity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86552040",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['sentiment'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74006d",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION/ MODELING \n",
    "\n",
    "determine features to be applied and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['categories', 'city', 'state', 'postal_code', 'is_open', 'text_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "catFeat = df[targets].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "catFeat['categories'] = le.fit_transform(catFeat['categories'].astype(str))\n",
    "catFeat['city'] = le.fit_transform(catFeat['city'].astype(str))\n",
    "catFeat['state'] = le.fit_transform(catFeat['state'].astype(str))\n",
    "catFeat['postal_code'] = le.fit_transform(catFeat['postal_code'].astype(str))\n",
    "\n",
    "catFeat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b041cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 30% test and 70% training\n",
    "X_train, X_test, y_train, y_test = train_test_split(catFeat, \n",
    "                                                    df['target'], \n",
    "                                                    test_size=0.3, random_state=0)\n",
    "\n",
    "# Create a random forest classifier for feature importance\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the name and gini importance of each feature\n",
    "for feature in zip(targets, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efea61a",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "\n",
    "Based on the RandomForest test for classifier importance, `categories`, `postal_code`, and `text_len` are the most important features adding up to about `0.835` of the total importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "y = df['target'].values\n",
    "\n",
    "x = catFeat.values\n",
    "\n",
    "chi_scores = chi2(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca938a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporess scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "\n",
    "p_values = pd.Series(chi_scores[1], index = catFeat.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142a304",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "For Based on the `p-values < 0.05` there is no dependence between any of the target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fa182",
   "metadata": {},
   "source": [
    "## Sentiment Based Prediction Model:\n",
    "\n",
    "Next, we'll create a Supervised ML model to predict whether a customer will recommend a product based on the text from the review and the sentiment of that text, and the length of the review.\n",
    "\n",
    "To create our model, we will be mixing both text and numeric values.  There are multiple ways to accomplish this, but we will be using a `ColumnTransformer` in a Pipeline[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ee094",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "During this section, we will use the ColumnTransformer to perform all of the pre-processing steps.  A `FeatureUnion` could also be used but isn't required since we're not transforming the entire numeric data with a PCA process [2].\n",
    "\n",
    ">*FeatureUnion applies different transformers to the whole of the input data and then combines the results by concatenating them.*  \n",
    ">*ColumnTransformer, on the other hand, applies different transformers to different subsets of the whole input data, and again concatenates the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['categories', 'postal_code', 'text', 'sentiment', 'text_len']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(clf):\n",
    "    '''Create a pipeline for a given classifier.  The classifier needs to be an instance\n",
    "    of the classifier with all parmeters needed specified.'''\n",
    "    \n",
    "    # Each pipeline uses the same column transformer.  \n",
    "    column_trans = ColumnTransformer(\n",
    "        [('Text', TfidfVectorizer(stop_words='english'), 'text'),\n",
    "         ('Categories', TfidfVectorizer(stop_words='english'), 'categories'),\n",
    "         ('postal_code', OneHotEncoder(dtype='int', handle_unknown='ignore'),['postal_code']),\n",
    "         ('Text Length', MinMaxScaler(), ['text_len']),\n",
    "         ('Sentiment', MinMaxScaler(), ['sentiment'])],\n",
    "        remainder='drop') \n",
    "    \n",
    "    pipeline = Pipeline([('prep',column_trans),\n",
    "                         ('over', SMOTE(random_state=42)),\n",
    "                         ('under', RandomUnderSampler(random_state=42)),\n",
    "                         ('clf', clf)])\n",
    "     \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'ComplementNB' : ComplementNB(),\n",
    "          #'SVC' : SVC(class_weight='balanced', random_state=42),\n",
    "          #'RandomForest' : RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "          'LogReg' : LogisticRegression(random_state=42, class_weight='balanced', max_iter=500)}\n",
    "\n",
    "for name, model, in models.items():\n",
    "    clf = model\n",
    "    pipeline = create_pipe(clf)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    %time scores = cross_val_score(pipeline, X, y, scoring='f1_macro', cv=cv, n_jobs=6, error_score='raise')\n",
    "    print(name, ': Mean f1 Macro: %.3f and Standard Deviation: (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08a1dd",
   "metadata": {},
   "source": [
    "# PERFORMANCE ASSESSMENT\n",
    "\n",
    "assess model performance using appropriate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e5988",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d757b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(parameters, X, y, pipeline):\n",
    "    ''' implements a the GridSearch Cross validation for a given model and set of parameters'''\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "    grid = GridSearchCV(pipeline, parameters, scoring='f1_macro', n_jobs=-1, cv=cv, error_score='raise')\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f989306",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'clf__solver' : ['newton-cg', 'lbfgs', 'sag', 'liblinear'],\n",
    "               'clf__C' : [1, 10 ,100]}]\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=500)\n",
    "pipeline = create_pipe(clf)\n",
    "grid = get_params(parameters, X_train, y_train, pipeline)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test, y_test))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "log_C = grid.best_params_['clf__C']\n",
    "log_solver = grid.best_params_['clf__solver']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9707260",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4488c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_print(pipeline, name):\n",
    "    ''' take a supplied pipeline and run it against the train-test spit \n",
    "    and product scoring results.'''\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    score = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    plot_confusion_matrix(cm, classes=[0,1], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad30de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=log_C, solver=log_solver, random_state=42, max_iter=500)\n",
    "pipeline = create_pipe(clf)\n",
    "fit_and_print(pipeline, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea22d6",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "This model performed extremely well across our dataset with an `f1 macro` score of `0.822`.  The dataset has imbalanced, which was corrected for SMOTE (Over Sampling combined with Undersampling).  The result was a very strong predictor model based on `categories`, `city`, `state`, `text`, `sentiment`, `text_len`.\n",
    "\n",
    "**Synthetic Minority Oversampling Technique** uses a nearest-neighbor approach for generating new minority class samples.  The method is applied only to the training data and then tested on the original, untouched test partition.  The method chosen here is first to oversample the minority class making it balanced, and then undersample it to reduce the size.  This helps bring balance without bloating the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740007ef",
   "metadata": {},
   "source": [
    "## Multi-Class Classification Model for Category Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65503ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories'] = df['categories'].apply(lambda w: re.sub(' ', '', w))\n",
    "df['categories'] = df['categories'].apply(lambda w: w.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['text', 'categories']]\n",
    "y = df['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(mlb.classes_[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(clf):\n",
    "    '''Create a pipeline for a given classifier.  The classifier needs to be an instance\n",
    "    of the classifier with all parmeters needed specified.'''\n",
    "    \n",
    "    # Each pipeline uses the same column transformer.  \n",
    "    column_trans = ColumnTransformer(\n",
    "        [('text', TfidfVectorizer(stop_words='english'), 'text')],\n",
    "        remainder='drop') \n",
    "    \n",
    "    pipeline = Pipeline([('prep',column_trans), \n",
    "                         ('clf', clf)])\n",
    "     \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RF' : OneVsRestClassifier(RandomForestClassifier(), n_jobs=-1),\n",
    "          'LogReg' :LogisticRegression(LogisticRegression(), n_jobs=-1)}\n",
    "\n",
    "for name, model, in models.items():\n",
    "    clf = model\n",
    "    pipeline = create_pipe(clf)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='f1_macro', cv=3, n_jobs=-1, error_score='raise')\n",
    "    print(name, ': Mean f1 Macro: %.3f and Standard Deviation: (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39a327",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "Again the `SVC` classifier performed the best with `LogisticRegression` coming out second best.   `MultinomialNB` perfrormed the worst out of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e8204",
   "metadata": {},
   "source": [
    "### Multi-Class Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66780ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Optimization was performed prior to Hyperparemeter selection\n",
    "clf = OneVsRestClassifier(LogisticRegression(class_weight='balanced', random_state=42))\n",
    "pipeline = create_pipe(clf)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "score = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627e569",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da518f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the text lables from the MultiLabelBinarizer\n",
    "pred_labels = mlb.inverse_transform(y_pred)\n",
    "\n",
    "# Append them to the DataFrame\n",
    "X_test['Predicted Labels'] = pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b94cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of them\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "X_test.sample(10, random_state=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b844325-e081-4bbe-b640-a2464ac0a816",
   "metadata": {},
   "source": [
    "## Neural Network for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b28600-29a0-48a3-b2b6-627f03ee46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa10a43-f62d-4202-8f7d-b2e18bfd1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97786fbd-1eb0-4801-ac8e-43fe6e7e3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26252ab4-82da-4de9-a64e-1d16f6287f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d726-401d-4595-9a52-7169e8713017",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy(copy=True)\n",
    "y_train = y_train.to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd42c8-550e-402b-9404-c2613fff5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization()\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd1c2b-8aaf-4adb-8c51-6871d79fcd95",
   "metadata": {
    "id": "SCIg_T50wOCU"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fc1dc-2e2a-4033-85c9-1a05c5376810",
   "metadata": {
    "id": "XULcm6B3xQIO"
   },
   "outputs": [],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "first_review = X_train[0]\n",
    "first_label = y_train[0]\n",
    "print(\"Review:\", first_review)\n",
    "print(\"Label:\", y_train[0])\n",
    "print(\"Vectorized Review:\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f213c90-6118-4196-ba74-fd323aa93fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1297])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[3017])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7d41d-02df-4f35-a79d-82147b8ed6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = vectorize_text(X_train, y_train)\n",
    "X_te, y_te = vectorize_text(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e6e8e-b1f5-4f75-9fe7-14ce6772a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the vocabulary to understand the input feature boundary.\n",
    "max_features = len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bd587-f680-464a-ac32-bd5cf0d42e1f",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "### Create the model\n",
    "\n",
    "It's time to create our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29666b-5b7b-4422-b268-35ffd1b51a69",
   "metadata": {
    "id": "dkQP6in8yUBR"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ec9f6-f81a-4144-b45b-dadbeec1e402",
   "metadata": {
    "id": "xpKOoWgu-llD"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d929688-d166-4fe7-973b-8c2fed9d49eb",
   "metadata": {
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a42ad0-6480-4713-ad36-2414deee211c",
   "metadata": {
    "id": "35jv_fzP-llU"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "You will train the model by passing the `dataset` object to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d07f11-7bc5-491b-9e81-70f5a8edd599",
   "metadata": {
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "history = model.fit(x = X_tr, y = y_tr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e22cd-3526-4b0a-90b2-f8b1514540bb",
   "metadata": {
    "id": "zOMKywn4zReN"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x = X_te, y = y_te)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5c340-9502-4d9e-8b68-7a1e4a84d8b1",
   "metadata": {
    "id": "-YcvZsdvWfDf"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72a8f2-21bb-42ac-9b9b-0bd827fc32ce",
   "metadata": {
    "id": "1_CH32qJXruI"
   },
   "source": [
    "There are four entries: one for each monitored metric during training and validation. You can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41182a3-9159-40b4-880d-55e8a769085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509c969-f17b-4fef-a659-8c72a2462694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'g:', label='Training acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0a61b-1a9a-4fde-9074-4125b16cceaa",
   "metadata": {
    "id": "2SEMeQ5YXs8z"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r:', label='Training loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665a220-da6c-4bf3-9263-681190e54aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  \"The food was great!\",\n",
    "  \"The service was okay.\",\n",
    "  \"The nail salon was terrible...\",\n",
    "  \"The food was good, the service was terrible, the decor was spectacular.\",\n",
    "  \"Wow this thing is amazing!\"\n",
    "  ]\n",
    "\n",
    "vector_examples = vectorize_layer(examples)\n",
    "\n",
    "res = model.predict(vector_examples)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bd851-259d-409f-93bf-617d249ac696",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- The values above indicate a `postive` value as a postive sentiment and `negative` as a negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bc98a",
   "metadata": {},
   "source": [
    "# CONCLUSION \n",
    "\n",
    "a conclusion summarizing the analysis and the results. Were you able to meet your analysis objective as described in your introduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bca75",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "1. [Yelp Reviews Dataset](https://www.yelp.com/dataset)\n",
    "1. [Basic Text Classification with TensorFlow](https://www.tensorflow.org/tutorials/keras/text_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8373b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
