{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitted-tokyo",
   "metadata": {},
   "source": [
    "# Week 9 Extra Credit: Basic IR System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-olive",
   "metadata": {},
   "source": [
    "## Basic Text preprocessing steps\n",
    "removing noise: anything that isn’t a standard number or letter\n",
    "removing stop words: very common words that add little value in analysis are removed from the vocabulary.\n",
    "stemming: reducing inflected (or derived) words to their stem, base or root form \n",
    "lemmatization: similar to stemming, however stemming can often create non-words, whereas lemmas are actual words\n",
    "\n",
    "## Bag of Words (BoW) Model\n",
    "After preprocessing, text needs to be transformed into a meaningful number vectors for use in ML algorithms. The BoW model represents text as a matrix of word counts within a document. It's called a “bag of words\" because information about the order or structure of words is discarded. The model only cares whether the known words occur in the document, but not where they occur. Intuitively, documents are similar if they have similar content.\n",
    "It involves:\n",
    " - a vocabulary of known words\n",
    " - a measure of the presence of known words\n",
    "\n",
    "For example, given a dictionary containing {Learning, is, the, not, great}, to vectorize the text “Learning is great”.\n",
    "Its vector representation would be : $(1, 1, 0, 0, 1)$, where the numbers represent their word counts.\n",
    "\n",
    "## TF-IDF\n",
    "With BoW, highly frequent words start to dominate the document, but such words may not contain much informational content. It also gives more weight to longer documents than shorter documents.  \n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents. The scores for frequent words that are also frequent across all documents are penalized. This scoring is called Term Frequency-Inverse Document Frequency, where\n",
    " - Term Frequency: a scoring of the frequency of the word in the current document\n",
    "    - TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    " - Inverse Document Frequency: a scoring of how rare a word is across documents.\n",
    "    - IDF = $1+log(N/n)$, where, $N$ is the number of documents and n is the number of documents a term $t$ has appeared in.\n",
    "\n",
    "## Cosine Similarity\n",
    "A measure of similarity between two non-zero vectors of an inner product space</ul>\n",
    " - Tf-idf weight is a weight often used in information retrieval (IR) and text mining.\n",
    " - It is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "     - Cosine Similarity $(d1, d2)= Dot product (d1, d2) / ||d1|| * ||d2||$ where $d1,d2$ are two non zero vectors.\n",
    "     \n",
    "**Reference:**  \n",
    " 1. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    " 1. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    " 1. https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    " 1. http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suspected-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "piano-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Trump became president after winning the political election. \\\n",
    "    Though he lost the support of some republican friends, Trump is friends with President Putin\",\n",
    "\n",
    "    \"President Trump says Putin had no political interference in the election outcome. \\\n",
    "    He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing \\\n",
    "    to do with the election\",\n",
    "    \n",
    "    \"The COVID-19 pandemic continues to ravage populations around the globe. Worldwide over 2.5 million have died. \\\n",
    "    In the United States the death toll is over half a million\",\n",
    "\n",
    "    \"Post elections, Vladimir Putin became President of Russia. \\\n",
    "    President Putin had served as the Prime Minister earlier in his political career\",\n",
    "    \n",
    "    \"Experts point out that the COVID-19 vaccines have nothing to do with the reproductive system, and that the vaccines \\\n",
    "    do not use live viruses or alter human DNA.\",\n",
    "    \n",
    "    \"President Joe Biden said Tuesday that the U.S. expects to take delivery of enough coronavirus vaccines for\\\n",
    "    all adult Americans by the end of May, two months earlier than anticipated, as his administration \\\n",
    "    announced that drugmaker Merck & Co. will help produce rival Johnson & Johnson’s newly approved shot.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-lobby",
   "metadata": {},
   "source": [
    "Text preprocessing helper functions which will be passed as a parameter to or Vectorization object.\n",
    " - removes punctionation and special characters\n",
    " - lower-cases\n",
    " - stemming or lemmatization; in this case I'll choose lemmatization as stemming can result in 'non-words'\n",
    "\n",
    "## Find base form of a word (eg: stemming or lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chubby-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_lemmer = WordNetLemmatizer()\n",
    "ps_stemmer = PorterStemmer()\n",
    "\n",
    "#  custom tokenizer\n",
    "def custom_tokenizer(str_input):\n",
    "   \n",
    "    # remove special characters and stem\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    \n",
    "    #words = [ps_stemmer.stem(word) for word in words]    # stemmer\n",
    "    words = [wn_lemmer.lemmatize(word) for word in words] # lemmatizer\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-thompson",
   "metadata": {},
   "source": [
    "To compute the cosine similarity, you need the word count of the words in each document. We will use TF-IDF\n",
    " - With TfidfVectorizer or CountVectorizer, a tokenizer can be specified that allows for custom preprocessing of the words.\n",
    "\n",
    "## First let's look at CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loving-participation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>administration</th>\n",
       "      <th>adult</th>\n",
       "      <th>alter</th>\n",
       "      <th>american</th>\n",
       "      <th>announced</th>\n",
       "      <th>anticipated</th>\n",
       "      <th>approved</th>\n",
       "      <th>biden</th>\n",
       "      <th>...</th>\n",
       "      <th>u</th>\n",
       "      <th>united</th>\n",
       "      <th>use</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>virus</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>wa</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "      <th>worldwide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2  5  administration  adult  alter  american  announced  anticipated  \\\n",
       "0  0  0               0      0      0         0          0            0   \n",
       "1  0  0               0      0      0         0          0            0   \n",
       "2  1  1               0      0      0         0          0            0   \n",
       "3  0  0               0      0      0         0          0            0   \n",
       "4  0  0               0      0      1         0          0            0   \n",
       "5  0  0               1      1      0         1          1            1   \n",
       "\n",
       "   approved  biden  ...  u  united  use  vaccine  virus  vladimir  wa  \\\n",
       "0         0      0  ...  0       0    0        0      0         0   0   \n",
       "1         0      0  ...  0       0    0        0      0         0   1   \n",
       "2         0      0  ...  0       1    0        0      0         0   0   \n",
       "3         0      0  ...  0       0    0        0      0         1   0   \n",
       "4         0      0  ...  0       0    1        2      1         0   0   \n",
       "5         1      1  ...  1       0    0        1      0         0   0   \n",
       "\n",
       "   winning  witchhunt  worldwide  \n",
       "0        1          0          0  \n",
       "1        0          1          0  \n",
       "2        0          0          1  \n",
       "3        0          0          0  \n",
       "4        0          0          0  \n",
       "5        0          0          0  \n",
       "\n",
       "[6 rows x 76 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVec = CountVectorizer(stop_words='english', tokenizer=custom_tokenizer)\n",
    "X = countVec.fit_transform(docs)\n",
    "\n",
    "# display as a dataframe\n",
    "df = pd.DataFrame(X.toarray(), columns=countVec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-exemption",
   "metadata": {},
   "source": [
    "We can see vaccine appeared 2X in doc 5 (eg: index by 0) and 1X in doc 6.  \n",
    "\n",
    "doc 5: \"Experts point out that the COVID-19 vaccines have nothing to do with the reproductive system, and that the vaccines do not use live viruses or alter human DNA.\"  \n",
    "\n",
    "We can also see 2 and 5 appearing in doc 3: \"The COVID-19 pandemic continues to ravage populations around the globe. Worldwide over 2.5 million have died. In the United States the death toll is over half a million\"  \n",
    "\n",
    "Due to remval of punctuation during preprocessing, 2.5 was transformed into 2 and 5. This is an example of some of the challenges of preprocessing. Similarly, in doc 6, U.S. is transformed to u and s. In this case, we need a more sophisticated regular expression to handle this scenario.  \n",
    "\n",
    "## Next lets use use TF-IDF which account how often a term shows up in determining importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diagnostic-vacation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>administration</th>\n",
       "      <th>adult</th>\n",
       "      <th>alter</th>\n",
       "      <th>american</th>\n",
       "      <th>announced</th>\n",
       "      <th>anticipated</th>\n",
       "      <th>approved</th>\n",
       "      <th>biden</th>\n",
       "      <th>...</th>\n",
       "      <th>u</th>\n",
       "      <th>united</th>\n",
       "      <th>use</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>virus</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>wa</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "      <th>worldwide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232469</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.231419</td>\n",
       "      <td>0.231419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284416</td>\n",
       "      <td>0.466450</td>\n",
       "      <td>0.284416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2         5  administration     adult     alter  american  \\\n",
       "0  0.000000  0.000000        0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000        0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.231419  0.231419        0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000        0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000        0.000000  0.000000  0.284416  0.000000   \n",
       "5  0.000000  0.000000        0.174883  0.174883  0.000000  0.174883   \n",
       "\n",
       "   announced  anticipated  approved     biden  ...         u    united  \\\n",
       "0   0.000000     0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "1   0.000000     0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2   0.000000     0.000000  0.000000  0.000000  ...  0.000000  0.231419   \n",
       "3   0.000000     0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "4   0.000000     0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "5   0.174883     0.174883  0.174883  0.174883  ...  0.174883  0.000000   \n",
       "\n",
       "        use   vaccine     virus  vladimir        wa   winning  witchhunt  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.286005   0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.232469  0.000000   0.232469   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.289205  0.000000  0.000000   0.000000   \n",
       "4  0.284416  0.466450  0.284416  0.000000  0.000000  0.000000   0.000000   \n",
       "5  0.000000  0.143406  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   worldwide  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.231419  \n",
       "3   0.000000  \n",
       "4   0.000000  \n",
       "5   0.000000  \n",
       "\n",
       "[6 rows x 76 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVec = TfidfVectorizer(stop_words='english',        # remove stop words\n",
    "                           tokenizer=custom_tokenizer   # preprocessing\n",
    "                          )\n",
    "tfidf = TfidfVec.fit_transform(docs)\n",
    "\n",
    "\n",
    "# display as a dataframe\n",
    "df = pd.DataFrame(tfidf.toarray(), columns=TfidfVec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-definition",
   "metadata": {},
   "source": [
    "Instead of just being a count of words, it’s the percentage of the words.  \n",
    "\n",
    "Looking again at doc 5: \"Experts point out that the COVID-19 vaccines have nothing to do with the reproductive system, and that the vaccines do not use live viruses or alter human DNA.\"  \n",
    "\n",
    "We can see `vacine` represents `46.6%` , `use` represents `28%` and `virus` is `28%`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-raleigh",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "Let's calculate similarity of documents using cosine_similarity, given our vectorized corpus. Suppose we had a question whose answer is listed in our corpus. This is a simplied example of IR (information retrieval)\n",
    "\n",
    "1. first add the query to our corpus\n",
    "1. then compare the similarity of the query to the existing corpus, using cosine_similarity function\n",
    "1. then return the document that is most similar to our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solved-london",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17482074, 0.28689806, 0.        , 0.35970216, 0.        ,\n",
       "        0.        , 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our query\n",
    "newDoc = 'Who is Putin?'\n",
    "\n",
    "# step 1\n",
    "docs.append(newDoc)  # add our query to our corpus\n",
    "\n",
    "# step 2\n",
    "# need to re-vectorize given query addition\n",
    "TfidfVec = TfidfVectorizer(stop_words='english', tokenizer=custom_tokenizer)\n",
    "tfidf = TfidfVec.fit_transform(docs)\n",
    "    \n",
    "# compare the similarity of the query to the existing corpus    \n",
    "vals = cosine_similarity(tfidf[-1],  # query. (eg: the last doc via append\n",
    "                             tfidf)  # all documents\n",
    "\n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-cookie",
   "metadata": {},
   "source": [
    "Let's look at this closer\n",
    " - cosine_similarity compares consine simiarlity of 2 docs\n",
    " - cosine similarity of 2 documents will range from 0 to 1  \n",
    "\n",
    "The following looks at the cosine similarity between our query and each of the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boring-wrestling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs shape: (7, 76) \n",
      "\n",
      "similarity between the query and existing docs\n",
      "[[0.17482074]]\n",
      "[[0.28689806]]\n",
      "[[0.]]\n",
      "[[0.35970216]]\n",
      "[[0.]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# this includes our query\n",
    "print('docs shape:', tfidf.shape, '\\n')  \n",
    "\n",
    "# original docs\n",
    "d1, d2, d3, d4, d5, d6 = tfidf[0], tfidf[1], tfidf[2], tfidf[3], tfidf[4], tfidf[5]\n",
    "\n",
    "# our query\n",
    "q = tfidf[6]\n",
    "\n",
    "print('similarity between the query and existing docs')\n",
    "print(f\"{cosine_similarity(q,d1)}\\n{cosine_similarity(q,d2)}\\n{cosine_similarity(q,d3)}\\n{cosine_similarity(q, d4)}\\n{cosine_similarity(q,d5)}\\n{cosine_similarity(q,d6)}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-channel",
   "metadata": {},
   "source": [
    "We can see that doc 4 at index 3 is the most similar.  \n",
    "\n",
    "The following describes how to pull out that index from the cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civil-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query Who is Putin? \n",
      "\n",
      "[[2 4 5 0 1 3 6]]\n",
      "(1, 7)\n",
      "\n",
      "[2 4 5 0 1 3 6]\n",
      "index: 3\n",
      "\n",
      "(7,)\n",
      "\n",
      "0.3597021557041277\n",
      "\n",
      "Post elections, Vladimir Putin became President of Russia.     President Putin had served as the Prime Minister earlier in his political career\n"
     ]
    }
   ],
   "source": [
    "print('query', newDoc, '\\n') # remind ourselves of the query\n",
    "\n",
    "print(vals.argsort())       # returns the indexes for the values in sorted order\n",
    "print(vals.argsort().shape) # it's dimension/shape\n",
    "\n",
    "print()\n",
    "print(vals.argsort()[0])     # get a list of indexes of the sorted array\n",
    "\n",
    "idx = vals.argsort()[0][-2]  # get the index of the highest value (note: -2 is the last doc, since -1 is the query\n",
    "print('index:', idx)\n",
    "\n",
    "print()\n",
    "flat = vals.flatten()        # flatten the array\n",
    "flat.sort()\n",
    "print(flat.shape)\n",
    "\n",
    "print()\n",
    "req_tfidf = flat[-2]        # pull out the similarity value\n",
    "print(req_tfidf)\n",
    "\n",
    "# show the doc that is most similar to our query\n",
    "print()\n",
    "if(req_tfidf==0):\n",
    "    print(\"We didn't find any matches, please try again.\")\n",
    "else:\n",
    "    print( docs[idx])\n",
    "    docs.remove(newDoc) # need to remove the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-mapping",
   "metadata": {},
   "source": [
    "Putting it all together to get the index of the document that is most similar to our queru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "backed-maker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3597021557041277"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = vals.argsort()[0][-2]  # index\n",
    "\n",
    "flat = vals.flatten()\n",
    "flat.sort()\n",
    "\n",
    "req_tfidf = flat[-2]\n",
    "req_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
