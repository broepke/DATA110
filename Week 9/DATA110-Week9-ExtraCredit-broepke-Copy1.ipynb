{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abandoned-desert",
   "metadata": {},
   "source": [
    "# Week 9 Extra Credit: Basic IR System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liquid-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-fundamentals",
   "metadata": {},
   "source": [
    "# Sonoma Wine Reccomendation System\n",
    "\n",
    "Using a subset of the wines in the Wine Spectator Kaggle dataset, create a reccomendation system based on the closet match of a user inputted description.\n",
    "\n",
    "https://www.kaggle.com/zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serial-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bridal-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Malbec, Merlot and Petit Verdot are included in this wine that's black-purple in color, with a thick density to its dusty black fruit. Cedar, tar and tobacco weave in between blackberry and currant, finishing in a powerful grip of tannin.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = list(wine_df['description'].values)\n",
    "wine[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suspended-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_lemmer = WordNetLemmatizer()\n",
    "ps_stemmer = PorterStemmer()\n",
    "\n",
    "#  custom tokenizer\n",
    "def custom_tokenizer(str_input):\n",
    "   \n",
    "    # remove special characters and stem\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    \n",
    "    #words = [ps_stemmer.stem(word) for word in words]    # stemmer\n",
    "    words = [wn_lemmer.lemmatize(word) for word in words] # lemmatizer\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-billy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-</th>\n",
       "      <th>-ness</th>\n",
       "      <th>-type</th>\n",
       "      <th>0</th>\n",
       "      <th>000</th>\n",
       "      <th>000-feet</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>...</th>\n",
       "      <th>zinfandoodle</th>\n",
       "      <th>zingier</th>\n",
       "      <th>zinginess</th>\n",
       "      <th>zingy</th>\n",
       "      <th>zinny</th>\n",
       "      <th>zins</th>\n",
       "      <th>zio</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippy</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11255</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11258 rows × 8127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         -  -ness  -type    0  000  000-feet   01   02   03   04  ...  \\\n",
       "0      0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1      0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2      0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3      0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4      0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...    ...    ...    ...  ...  ...       ...  ...  ...  ...  ...  ...   \n",
       "11253  0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "11254  0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "11255  0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "11256  0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "11257  0.0    0.0    0.0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "       zinfandoodle  zingier  zinginess  zingy  zinny  zins  zio  zip  zippy  \\\n",
       "0               0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "1               0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "2               0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "3               0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "4               0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "...             ...      ...        ...    ...    ...   ...  ...  ...    ...   \n",
       "11253           0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "11254           0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "11255           0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "11256           0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "11257           0.0      0.0        0.0    0.0    0.0   0.0  0.0  0.0    0.0   \n",
       "\n",
       "       zone  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "...     ...  \n",
       "11253   0.0  \n",
       "11254   0.0  \n",
       "11255   0.0  \n",
       "11256   0.0  \n",
       "11257   0.0  \n",
       "\n",
       "[11258 rows x 8127 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVec = TfidfVectorizer(stop_words='english',        # remove stop words\n",
    "                           tokenizer=custom_tokenizer   # preprocessing\n",
    "                          )\n",
    "tfidf = TfidfVec.fit_transform(wine)\n",
    "\n",
    "\n",
    "# display as a dataframe\n",
    "df = pd.DataFrame(tfidf.toarray(), columns=TfidfVec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-paradise",
   "metadata": {},
   "source": [
    "## Let the User Enter a Value and Return a Wine Reccomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dress-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wine_reccomendation(new_wine):\n",
    "\n",
    "    # step 1\n",
    "    wine.append(new_wine)  # add our query to our corpus\n",
    "\n",
    "    # step 2\n",
    "    # need to re-vectorize given query addition\n",
    "    TfidfVec = TfidfVectorizer(stop_words='english', tokenizer=custom_tokenizer)\n",
    "    tfidf = TfidfVec.fit_transform(wine)\n",
    "\n",
    "    # compare the similarity of the query to the existing corpus    \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "\n",
    "    idx = vals.argsort()[0][-2]  # get the index of the highest value (note: -2 is the last doc, since -1 is the query\n",
    "\n",
    "    flat = vals.flatten()        # flatten the array\n",
    "    flat.sort()\n",
    "\n",
    "    req_tfidf = flat[-2]        # pull out the similarity value\n",
    "\n",
    "    # show the doc that is most similar to our query\n",
    "    if(req_tfidf==0):\n",
    "        print(\"We didn't find any matches, please try again.\")\n",
    "    else:\n",
    "        print(\"Similarity Score =\", req_tfidf)\n",
    "        print(wine_df.loc[idx])\n",
    "        wine.remove(new_wine) # need to remove the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unique-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score = 0.2722102531573034\n",
      "country                                                       US\n",
      "description    Velvety and plush, it's 100% Cabernet Sauvigno...\n",
      "designation                                             Optimism\n",
      "points                                                        87\n",
      "price                                                       49.0\n",
      "province                                              California\n",
      "region_1                                           Sonoma County\n",
      "region_2                                                  Sonoma\n",
      "variety                                       Cabernet Sauvignon\n",
      "winery                                          Pangloss Cellars\n",
      "Name: 8311, dtype: object\n"
     ]
    }
   ],
   "source": [
    "wine_reccomendation(\"cherry chocolate notes of walnut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "premier-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your wine flavor preferences or \"QUIT\" to exit:  walnut, coffee, cherries, apples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walnut, coffee, cherries, apples\n",
      "Similarity Score = 0.25608295825642285\n",
      "country                                                       US\n",
      "description    Velvety and plush, it's 100% Cabernet Sauvigno...\n",
      "designation                                             Optimism\n",
      "points                                                        87\n",
      "price                                                       49.0\n",
      "province                                              California\n",
      "region_1                                           Sonoma County\n",
      "region_2                                                  Sonoma\n",
      "variety                                       Cabernet Sauvignon\n",
      "winery                                          Pangloss Cellars\n",
      "Name: 8311, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your wine flavor preferences or \"QUIT\" to exit:  quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Get the Genre from the user or abort\n",
    "    w = input('Enter your wine flavor preferences or \"QUIT\" to exit: ')\n",
    "    if w.lower() == \"quit\":\n",
    "        # If the user enters quit, then exit the while loop completely.\n",
    "        break\n",
    "    else:\n",
    "        print(w)\n",
    "        wine_reccomendation(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-correction",
   "metadata": {},
   "source": [
    "## Basic Text preprocessing steps\n",
    "removing noise: anything that isn’t a standard number or letter\n",
    "removing stop words: very common words that add little value in analysis are removed from the vocabulary.\n",
    "stemming: reducing inflected (or derived) words to their stem, base or root form \n",
    "lemmatization: similar to stemming, however stemming can often create non-words, whereas lemmas are actual words\n",
    "\n",
    "## Bag of Words (BoW) Model\n",
    "After preprocessing, text needs to be transformed into a meaningful number vectors for use in ML algorithms. The BoW model represents text as a matrix of word counts within a document. It's called a “bag of words\" because information about the order or structure of words is discarded. The model only cares whether the known words occur in the document, but not where they occur. Intuitively, documents are similar if they have similar content.\n",
    "It involves:\n",
    " - a vocabulary of known words\n",
    " - a measure of the presence of known words\n",
    "\n",
    "For example, given a dictionary containing {Learning, is, the, not, great}, to vectorize the text “Learning is great”.\n",
    "Its vector representation would be : $(1, 1, 0, 0, 1)$, where the numbers represent their word counts.\n",
    "\n",
    "## TF-IDF\n",
    "With BoW, highly frequent words start to dominate the document, but such words may not contain much informational content. It also gives more weight to longer documents than shorter documents.  \n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents. The scores for frequent words that are also frequent across all documents are penalized. This scoring is called Term Frequency-Inverse Document Frequency, where\n",
    " - Term Frequency: a scoring of the frequency of the word in the current document\n",
    "    - TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    " - Inverse Document Frequency: a scoring of how rare a word is across documents.\n",
    "    - IDF = $1+log(N/n)$, where, $N$ is the number of documents and n is the number of documents a term $t$ has appeared in.\n",
    "\n",
    "## Cosine Similarity\n",
    "A measure of similarity between two non-zero vectors of an inner product space</ul>\n",
    " - Tf-idf weight is a weight often used in information retrieval (IR) and text mining.\n",
    " - It is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "     - Cosine Similarity $(d1, d2)= Dot product (d1, d2) / ||d1|| * ||d2||$ where $d1,d2$ are two non zero vectors.\n",
    "     \n",
    "**Reference:**  \n",
    " 1. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    " 1. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    " 1. https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    " 1. http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
