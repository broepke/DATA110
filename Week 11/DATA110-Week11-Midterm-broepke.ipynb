{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flexible-district",
   "metadata": {},
   "source": [
    "# Week 11 Midterm: NLP\n",
    "\n",
    "**Brian Roepke**  \n",
    "*DATA 110*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-attack",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "As a clothing retailer with an e-Commerce presence, it's important for us to understand what customers are saying about our products, which products are most popular with our customers, and how we can improve our offerings.  To accomplish this we'll take a look at several different analyses that will help with the following:\n",
    "\n",
    "1. **Review Trends**: General trends of customer reviews, quantity, distribution, etc.\n",
    "1. **Sentiment Analysis**:  How customers feel about the products; are they positive or negative generally.\n",
    "1. **Part of Speech Analysis**: Using different parts of speech (Nouns, Verbs, Adjectives, etc.).  Via this, we can see the most common positive and negative words used to describe products as well as which a the most commonly referenced products categories.\n",
    "1. **Recommendation Prediction**: We will use this customer sentiment to understand better if a customer will give a positive rating on the clothing items based on their review.\n",
    "1. **Department Prediction**: Finally, we'll use a multi-label classification model to predict the departments a product belongs to based on the description that's being used.  This might identify cross-selling opportunities or cross-listing opportunities for products in new categories.\n",
    "\n",
    "![](https://github.com/broepke/DATA110/blob/main/Week%2011/clothing.jpg?raw=true)\n",
    "<a href='https://www.freepik.com/vectors/woman'>Woman vector created by freepik - www.freepik.com</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn import metrics \n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# NLTK Imports and Downloads\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-senegal",
   "metadata": {},
   "source": [
    "# Preprocessing & EDA\n",
    "\n",
    "Importing our dataset and providing the necessary cleaning and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ClothingReviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-commodity",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-thomas",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "\n",
    "Nulll values are generally not desireable in a dataset.  In certain cases, observations (rows) with low counts will simply be dropped, in other cases, they can be filled with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan/null\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Department Name', 'Class Name', 'Review Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-sender",
   "metadata": {},
   "source": [
    "**Note**: The null values for the lower counts (except `Title`) were dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-terminal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill the NA values with 0\n",
    "df['Title'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-strip",
   "metadata": {},
   "source": [
    "**Note**: Any `NULL` values for the title field are filled with blank strings.  Next these will be combined with the `Review Text` field so we have a single text field for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Title'] + ' ' + df['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Title', 'Review Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column 'text_len' that counts the length for the derived field\n",
    "df['text_len'] = df.apply(lambda row: len(row['Text']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-intersection",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "A common practice is to review any duplicates.  If there are large quantities, they can skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "len_after = df.shape[0]\n",
    "\n",
    "print(\"Before =\", len_before)\n",
    "# drop duplicates\n",
    "print(\"After =\", len_after)\n",
    "print('')\n",
    "print(\"Total Removed =\", len_before - len_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-surge",
   "metadata": {},
   "source": [
    "**Note**: After the prior clean up of `NULL` values, there were just `2` duplicates left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-developer",
   "metadata": {},
   "source": [
    "### Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-recording",
   "metadata": {},
   "source": [
    "**Observations:** \n",
    "\n",
    "1. The age ranges for the dataset range from `18` to `99` with a mean of `43`.  Most shoppers are middled aged with our store.\n",
    "1. Ratings are based on `1-5` star system.  Mean rating is `4.18` meaning most people give positive reviews. \n",
    "1. Positive feedback count is the number of times that people found a review useful.  the mean is `2.63` with a min of `0` and max of `122`\n",
    "1. Text lenght will be covered later in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-experiment",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical data\n",
    "cat_data = df.select_dtypes(include=['object'])\n",
    "cat_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show counts values of each categorical variable\n",
    "for colname in cat_data.columns:\n",
    "    print (colname)\n",
    "    print (cat_data[colname].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-backup",
   "metadata": {},
   "source": [
    "**Observations:**  \n",
    "\n",
    "The categorical values are extermely clean and well labled.  We can look at the distributions of these better via visualization in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-therapy",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='Rating', data=df, palette=\"tab20\", dodge=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-integer",
   "metadata": {},
   "source": [
    "**Notes:** As observed in the prior section, the reviews are skewed to the postive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='Department Name', data=df, palette=\"tab20\", \n",
    "              order = df['Department Name'].value_counts().index, dodge=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-diving",
   "metadata": {},
   "source": [
    "**Notes**:  \n",
    "\n",
    "1. `Tops` followed by `Dresses` are the largest categories.  \n",
    "1. There are very few `Trend` and `Jackets` in the product line.  Predictions will be harder on these imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.countplot(x='Class Name', data=df, palette=\"tab20\", \n",
    "                   order = df['Class Name'].value_counts().index, dodge=False);\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.histplot(df, x='Positive Feedback Count', palette=\"tab20c\", binwidth=5);\n",
    "ax.set(yscale=\"log\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df, x='text_len', kde=True, palette=\"tab20c\", binwidth=10);\n",
    "# ax.set(yscale=\"log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-agent",
   "metadata": {},
   "source": [
    "**Notes**: For the `text_len` attribute, there is a fairly even distribution from `100` to `400` characters and then a higher concentration of reviews that are longer, around `500` characters.  Given that there are not massive outliers here, the number of characters of the reviews is probably limited to `~600~` chars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-exploration",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "For **Parts** of our analysis, the text needs to have some basic transformation for our models to work propertly.  These are as follows:\n",
    "\n",
    "1. **Lower**: Convert all characters to lowercase\n",
    "1. **Remove Punctuation**: In most cases, punctuation doesn't help NLP and ML models and can be removed.\n",
    "1. **Stop Word Removal**: Stop words generally don't add context to analysis (unless the length of text is very short (`100` - `200` characters) and can be removed.\n",
    "1. **Lemmatization**: Words will be reduced to there *Lemma* or root.  This will greatly improve the accuracy of the analysis since words like `simming` and `swimmer` will be reduced to `swim`.\n",
    "\n",
    "**Note**: The orginal text will be preserved for other analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text, stem=\"None\"):\n",
    "    \n",
    "    final_string = \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "    useless_words = useless_words + ['.', ',', '!', \"'\"]\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "    \n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "    \n",
    "    for word in text_stemmed:\n",
    "        final_string += word + \" \"\n",
    "    \n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_Processed'] = df['Text'].apply(lambda x: process_string(x, stem='Lem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_Processed'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-breath",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "For our sentiment analysis section, we will be using the `TextBlob` package to assist in creating `polarity scores` or sentiment scores that range from `-1` to `1` where lower scores are more negative and higher more positive.  Based off of these scores, we'll add a classifier of `1` for positive and `0` for negative to be used later in our prediction model. \n",
    "\n",
    "**Note**: `0` is technically nuetral sentiment, we'll verify how many observations were neutral before assuming we can use a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(x):\n",
    "    '''using TextBlob, get the sentiment score for a given body of text'''\n",
    "    blob = TextBlob(x)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Polarity Scoring from TextBlob\n",
    "df['sentiment'] = df['Text_Processed'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few extra columns to aid in the analysis\n",
    "df['sentiment_label'] = df['sentiment'].apply(lambda x: 1 if x >= 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[-4:]].sample(5, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df, x='sentiment', palette=\"tab20c\", bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-setting",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "The distributions of sentiment, similar to the `1-5` star reviews is left skewed to the positive.  There are very few that have a `<0` polartity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['sentiment'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-fight",
   "metadata": {},
   "source": [
    "**Note**: There are a small number (`83`) of reviews that received a `neutral` sentiment.  Since this number is so low, a `0` rating was grouped together with the majority class (`positive`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-asbestos",
   "metadata": {},
   "source": [
    "# Part-of-speech Tagging\n",
    "\n",
    "- show word counts for different parts of speech \n",
    "- What are popular products? identify nouns that can be used to tag the product (eg: dress, jacket, bottom, etc) and show their counts\n",
    "- Identify the top adjectives and adverbs for positive vs negative reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "df['Text_Tok'] = df['Text_Processed'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(x):\n",
    "    '''using TextBlob, get the full parsed results (POS, etc)'''\n",
    "    blob = TextBlob(x)\n",
    "    p = blob.parse()\n",
    "    p = re.sub(r'^\\w+/', '',p)\n",
    "    return p.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos(x):\n",
    "    '''pass a DataFrame column with tokenized text and return a DF of the Words'''\n",
    "    all_words = []\n",
    "    for l in x:\n",
    "        all_words = all_words + l\n",
    "        \n",
    "    df = pd.DataFrame(all_words)\n",
    "    df.columns = ['Word']\n",
    "    \n",
    "    # Add a column for the POS\n",
    "    df['Parse'] = df['Word'].apply(lambda x: parse_text(x))\n",
    "    \n",
    "    # Expned the extracted list of POS tags into their own columns, and concat that back to the orig DF\n",
    "    # https://chrisalbon.com/python/data_wrangling/pandas_expand_cells_containing_lists/\n",
    "    par = pd.DataFrame(df['Parse'].to_list(), columns=['P1','P2', 'P3', 'P4'])\n",
    "    df = pd.concat([df[:], par[:]], axis=1)\n",
    "    df.drop(columns=['Parse'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = build_pos(df['Text_Tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-gothic",
   "metadata": {},
   "source": [
    "**Notes:** Rather than using the much simpler approach of the POS with the TextBlog `tags` function[1], the `parse` function was used since it provides a more verpose labeling of the text.\n",
    "\n",
    "The attempt here was to try to discover if there was a better way to identify nouns that would better represent product features vs. other nouns.  Unfortunatley this didn't end up providing the detail needed. More information on this is presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-census",
   "metadata": {},
   "source": [
    "## Word Counts for Different Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_pos = df_words.groupby('P1')['P1'].count().\\\n",
    "    reset_index(name='count').sort_values(['count'],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data =df_top_pos, x='P1', y='count', palette=\"tab20\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-elizabeth",
   "metadata": {},
   "source": [
    "## Identify Top Product Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = df_words[df_words['P1'] == 'NN'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.groupby('Word')['Word'].count().reset_index(name='count').\\\n",
    "    sort_values(['count'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-norfolk",
   "metadata": {},
   "source": [
    "**Notes**:  When inspecting `nouns` only, there is a mix of different types of words displayed, and some we can see are not tagged in such a way that seems to make sense with this dataset.  For example, `love` is tagged as a noun, but it's probably an adjective.  `bit` is probably referring to an adjective as well but is showing as a noun.\n",
    "\n",
    "We can inspect these words directly to see if there is a difference in their POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob('dress').parse())\n",
    "print(TextBlob('love').parse())\n",
    "print(TextBlob('bit').parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-colleague",
   "metadata": {},
   "source": [
    "**Observations:**  When we try to use the Part of Speech (POS) tagging there isn't a distinction between Nouns.  Each of these have eactly the same POS sequence. \n",
    "\n",
    "We can use the Class name to determine clothing nouns to use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a list of all the unique class names\n",
    "noun_types = list(df['Class Name'].unique())\n",
    "\n",
    "# The words from the categories need to be lemmatized.\n",
    "lem = WordNetLemmatizer()\n",
    "for i in range(len(noun_types)):\n",
    "    noun_types[i] = lem.lemmatize(noun_types[i].lower())\n",
    "noun_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the text into a huge string and use Text Blobs to get a Dictionary out with counts\n",
    "all_text = ' '.join(df['Text_Processed'])\n",
    "all_text_blob = TextBlob(all_text)\n",
    "all_text_dict = all_text_blob.word_counts\n",
    "\n",
    "# Turn the dictionary into a Dataframe.  Filter by the word list and then sort for plotting.\n",
    "df_dict = pd.DataFrame(list(all_text_dict.items()),columns = ['Word','Count']) \n",
    "df_products = df_dict[df_dict.Word.isin(noun_types)]\n",
    "df_products.sort_values(by=['Count'], inplace=True, ascending=False)\n",
    "df_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-tomorrow",
   "metadata": {},
   "source": [
    "**Observations:**: Based on the top outputs we can see that `dresses` are the largest mentioned product line at a rate of `4x` the second, `sweaters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.barplot(x='Word', y='Count', data=df_products, palette=\"tab20\", dodge=False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-backup",
   "metadata": {},
   "source": [
    "## Top Adjectives and Adverbs for Positive vs Negative Reviews\n",
    "\n",
    "Using Part of Speech taggs, we can look at which adjectives and adverbs people are using most to describe the products.  Below is a table that shows how different parts of speech are encoded in this system.\n",
    " \n",
    "**Part of Speech Codes**\n",
    "\n",
    "<table align=\"left\" cellpadding=\"2\" cellspacing=\"2\" border=\"0\">\n",
    "  <tbody><tr bgcolor=\"#DFDFFF\" align=\"none\"> \n",
    "    <td align=\"none\"> \n",
    "      <div align=\"left\">Number</div>\n",
    "    </td>\n",
    "    <td> \n",
    "      <div align=\"left\">Tag</div>\n",
    "    </td>\n",
    "    <td> \n",
    "      <div align=\"left\">Description</div>\n",
    "    </td>\n",
    "  <tr bgcolor=\"#FFFFCA\"> \n",
    "    <td align=\"none\"> 7. </td>\n",
    "    <td>JJ </td>\n",
    "    <td>Adjective </td>\n",
    "  </tr>\n",
    "  <tr bgcolor=\"#FFFFCA\"> \n",
    "    <td align=\"none\"> 8. </td>\n",
    "    <td>JJR </td>\n",
    "    <td>Adjective, comparative </td>\n",
    "  </tr>\n",
    "  <tr bgcolor=\"#FFFFCA\"> \n",
    "    <td align=\"none\"> 9. </td>\n",
    "    <td>JJS </td>\n",
    "    <td>Adjective, superlative </td>\n",
    "  </tr>\n",
    "\n",
    "  <tr bgcolor=\"#FFFFCA\"> \n",
    "    <td align=\"none\"> 20. </td>\n",
    "    <td>RB </td>\n",
    "    <td>Adverb </td>\n",
    "  </tr>\n",
    "  <tr bgcolor=\"#FFFFCA\"> \n",
    "    <td align=\"none\"> 21. </td>\n",
    "    <td>RBR </td>\n",
    "    <td>Adverb, comparative </td>\n",
    "  </tr>\n",
    "  <tr bgcolor=\"#FFFFCA\"> \n",
    "    <td align=\"none\"> 22. </td>\n",
    "    <td>RBS </td>\n",
    "    <td>Adverb, superlative </td>\n",
    "  </tr>\n",
    "  \n",
    "</td></tr></tbody></table>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pos and neg reviews based on sentiment into their own DFs\n",
    "df_pos = df[df['sentiment_label'] == 1]\n",
    "df_neg = df[df['sentiment_label'] == 0]\n",
    "\n",
    "# Drop the rest of the columns after separating\n",
    "df_pos = df_pos[['Text_Tok']]\n",
    "df_neg = df_neg[['Text_Tok']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_mods(df_all_words):\n",
    "    ''' this function will return a dataframe of the top adjetives and \n",
    "    adverbs group together and counted'''\n",
    "    \n",
    "    df_mods = df_all_words[(df_all_words.P1.str.startswith('JJ')) | (df_all_words.P1.str.startswith('RB'))]\n",
    "\n",
    "    # Groupby, count, sort in order to get the counts of the words\n",
    "    df_grouped = df_mods.groupby(['P1', 'Word'])['Word'].count().\\\n",
    "        reset_index(name='count').sort_values(['count'],ascending=False)\n",
    "\n",
    "    # Convert to Multi-Level Index\n",
    "    df_grouped.set_index(['P1', 'Word'], inplace=True)\n",
    "\n",
    "    # Finally, just display the top 3 (if there are 3)\n",
    "    return df_grouped.groupby(level=0).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Dataframe via the Function\n",
    "df_all_words_pos = build_pos(df_pos['Text_Tok'])\n",
    "\n",
    "# Get the top words\n",
    "get_top_mods(df_all_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Dataframe via the Function\n",
    "df_all_words_neg = build_pos(df_neg['Text_Tok'])\n",
    "\n",
    "# Get the top words\n",
    "get_top_mods(df_all_words_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-expense",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "Above are the most frequently occuring positive and negative words per Adjective and Adverb.  \n",
    "\n",
    "1. **Positive**: Top words are `top`, `great`, `perfect`, `really`, and `pretty`\n",
    "1. **Negative**: Top words are `small`, `little`, `thin`, `tight`, and `short`\n",
    "\n",
    "None of the words really are suprising with the positive words, but with the negative words there appears to be a **sizing issue** where products are smaller or larger than people expect vs. the sizes claimed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-temperature",
   "metadata": {},
   "source": [
    "# Sentiment Based Prediction Model:\n",
    "\n",
    "Next we'll create a Supervised ML model to predict whether a product will be Recommended based on the text from the review as well as the sentiment of that text and the lenght of the review.\n",
    "\n",
    "To create our model we will be mixing both text and numeric values.  There are multiple ways to accomplish this but we will be using a `ColumnTransformer` in a Pipeline[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-france",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Text', 'sentiment', 'text_len']]\n",
    "y = df['sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_trans():\n",
    "    column_trans = ColumnTransformer(\n",
    "            [('Text', TfidfVectorizer(stop_words='english'), 'Text'),\n",
    "             ('Text Length', MinMaxScaler(), ['text_len']),\n",
    "             ('Sentiment', MinMaxScaler(), ['sentiment'])],\n",
    "            remainder='drop') \n",
    "    \n",
    "    return column_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(clf):\n",
    "    '''Create a pipeline for a given classifier.  The classifier needs to be an instance\n",
    "    of the classifier with all parmeters needed specified.'''\n",
    "    \n",
    "    # Each pipeline uses the same column transformer.  \n",
    "    column_trans = col_trans()\n",
    "    \n",
    "    pipeline = Pipeline([('prep',column_trans),\n",
    "                         ('over', SMOTE(random_state=42)),\n",
    "                         ('under', RandomUnderSampler(random_state=42)),\n",
    "                         ('clf', clf)])\n",
    "     \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'ComplementNB' : ComplementNB(),\n",
    "          'SVC' : SVC(class_weight='balanced', random_state=42),\n",
    "          'LogReg' : LogisticRegression(random_state=42, class_weight='balanced', max_iter=500),\n",
    "          'RandomForest' : RandomForestClassifier(class_weight='balanced', random_state=42)}\n",
    "\n",
    "for name, model, in models.items():\n",
    "    clf = model\n",
    "    pipeline = create_pipe(clf)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print(name, ': Mean f1 Macro: %.3f and Standard Deviation: (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-identity",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "The **Support Vector Machine** Classifier performed the best with the **Random Forest** and **Logistic Regression** behind it.  Complement Naive Bayes performed the worst. \n",
    "\n",
    "`SVC` is a fairly computationally expensive algorithm [5], it might be an advantages to use **Logistic Regression** if performance were top prioroty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-translator",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(parameters, X, y, pipeline):\n",
    "    ''' implements a the GridSearch Cross validation for a given model and set of parameters'''\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "    grid = GridSearchCV(pipeline, parameters, scoring='f1_macro', n_jobs=-1, cv=cv, error_score='raise')\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'clf__C': np.linspace(.1, 2 ,5), \n",
    "               'clf__gamma': [.01, .1, .5], \n",
    "               'clf__class_weight' : ['balanced']}]\n",
    "\n",
    "clf = SVC()\n",
    "pipeline = create_pipe(clf)\n",
    "grid = get_params(parameters, X_train, y_train, pipeline)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid.score(X_test, y_test))) \n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "svc_C = grid.best_params_['clf__C']\n",
    "svc_gamma = grid.best_params_['clf__gamma']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-profession",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_print(pipeline, name):\n",
    "    ''' take a supplied pipeline and run it against the train-test spit \n",
    "    and product scoring results.'''\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    score = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    plot_confusion_matrix(cm, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=svc_C, gamma=svc_gamma, class_weight='balanced', random_state=42)\n",
    "pipeline = create_pipe(clf)\n",
    "fit_and_print(pipeline, 'SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-contemporary",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "This model performed extermely well across our dataset with an `f1 macro` score of `0.915`.  The dataset does have imbalanced which was corrected for with SMOTE (Over Sampling combined with Undersampling).  The end result was a very strong predictor model based on `text`, `sentiment`, and `text_len`.\n",
    "\n",
    "Synthethic Minority Oversamlping Technique uses a nearest-neighbor approach for generating new minority class samples.  The method is applied only to the training data and then tested on the original, untouched test partition.  The method chosen here is to first oversample the minority class making it baalanced and then undersample it to reduce the size.  This helps bring balance without bloating the dataset [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-guest",
   "metadata": {},
   "source": [
    "## Test on Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(x):\n",
    "    '''calculate the numbers needed to run on custom data including sentiment and text length,\n",
    "    this is a farily simple process using the fuctions from previous transformations.'''\n",
    "    \n",
    "    x = process_string(x)\n",
    "    sent = get_sentiment(x)\n",
    "    length = len(x)\n",
    "    \n",
    "    d = {'Text' : x,\n",
    "         'sentiment' : sent,\n",
    "        'text_len' : length}\n",
    "\n",
    "    df = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = ['This dress is gorgeous and I love it and would gladly reccomend it to all of my friends.',\n",
    "        'This skirt has really horible quality and I hate it!',\n",
    "        'A super cute top with the perfect fit.',\n",
    "        'The most gorgeous pair of jeans I have seen.',\n",
    "        'this item is too little and tight.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The classifier will return 1 for Positive reviews and 0 for Negative reviews:','\\n')\n",
    "for rev in revs:\n",
    "    c_res = pipeline.predict(create_test_data(rev))\n",
    "    print(rev, '=', c_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-tsunami",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "Based on our custom strings, each one produced **Correct** classifications with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-religion",
   "metadata": {},
   "source": [
    "# Text Classification for Departments\n",
    "\n",
    "Next we'll attempt to create a Supervised Machine Learning model to classify products by Department.  This will look at the text that a user wrote in a review and determine what department the item came from.\n",
    "\n",
    "An interesting opportunity is to use this information to **cross sell** or **cross list** products.  If there is  a strong enough probability that an item could be in multiple departments from our analysis, could we **increase sales** with cross marketing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-slide",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "df['Department Name'] = df['Department Name'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Text', 'Department Name']]\n",
    "y = df['Department Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_trans():\n",
    "    column_trans = ColumnTransformer(\n",
    "            [('Text', TfidfVectorizer(stop_words='english'), 'Text')],\n",
    "            remainder='drop') \n",
    "    \n",
    "    return column_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(clf):\n",
    "    '''Create a pipeline for a given classifier.  The classifier needs to be an instance\n",
    "    of the classifier with all parmeters needed specified.'''\n",
    "    \n",
    "    # Each pipeline uses the same column transformer.  \n",
    "    column_trans = col_trans()\n",
    "    \n",
    "    pipeline = Pipeline([('prep',column_trans), \n",
    "                         ('over', SMOTE(random_state=42)),\n",
    "                         ('under', RandomUnderSampler(random_state=42)),\n",
    "                         ('clf', clf)])\n",
    "     \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'SVC' : OneVsRestClassifier(SVC(kernel='linear'), n_jobs=-1),\n",
    "          'RF' : OneVsRestClassifier(RandomForestClassifier(), n_jobs=-1),\n",
    "          'LogReg' : OneVsRestClassifier(LogisticRegression(), n_jobs=-1),\n",
    "          'Bayes' : OneVsRestClassifier(MultinomialNB(), n_jobs=-1)}\n",
    "\n",
    "for name, model, in models.items():\n",
    "    clf = model\n",
    "    pipeline = create_pipe(clf)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='f1_macro', cv=3, n_jobs=-1, error_score='raise')\n",
    "    print(name, ': Mean f1 Macro: %.3f and Standard Deviation: (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-medium",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "Again the `SVC` classifier performed the best with `LogisticRegression` coming out second best.   `MultinomialNB` perfrormed the worst out of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-cigarette",
   "metadata": {},
   "source": [
    "## Model Building & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Optimization was performed prior to Hyperparemeter selection\n",
    "clf = OneVsRestClassifier(SVC(C=.5, gamma=.1, kernel='linear', \n",
    "                              class_weight='balanced', random_state=42))\n",
    "pipeline = create_pipe(clf)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "score = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-choir",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    " - There are very few observations in the `5` class overall in the dataset.  You can see in the support column for this class only 38 samples are present.  Through synthetic oversampling (SMOTE), we could get a small precision and recall score, but it's difficult without more observations.\n",
    " - The `2` and `3` classes are also smaller and therefore do not have as high an `f1` score.\n",
    " - The remainder of the classes are all performing very well.  \n",
    " - The `f1 macro` score is `~.6`, which normally is not considered a good score, but in the case of multi-value classification, it doesn't necessarily mean it's a poor-performing model.  We can test this thorough inspection of the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the text lables from the MultiLabelBinarizer\n",
    "pred_labels = mlb.inverse_transform(y_pred)\n",
    "\n",
    "# Append them to the DataFrame\n",
    "X_test['Predicted Labels'] = pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-flashing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display a random sample of them\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "X_test.sample(10, random_state=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-stranger",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "In many casses our classifier was very accurate in determining the correct Department.  Since we used a multi-label classifier, the algorithm sugggested more than one label in some cases.  Upon inspecting these, in some cases the suggestion of an additional Deparment is incredibly logical.  \n",
    "\n",
    "It's possible to use these multi-label classes to investigate **cross marketing / cross listing** opportunities for these products.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-apple",
   "metadata": {},
   "source": [
    "## Test on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(x):\n",
    "    '''calculate the numbers needed to run on custom data including sentiment and text length,\n",
    "    this is a farily simple process using the fuctions from previous transformations.\n",
    "    ['Bottoms' 'Dresses' 'Intimate' 'Jackets' 'Tops' 'Trend']'''\n",
    "    \n",
    "    s = process_string(x[0])\n",
    "    \n",
    "    d = {'Text' : s,\n",
    "         'Department Name' : x[1]}\n",
    "\n",
    "    df = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "revs = [('This dress is gorgeous and would gladly reccomend it to all of my friends.', ['Dresses']),\n",
    "     ('These pants have really horible quality and I hate them!', ['Bottoms']),\n",
    "     ('A super cute blouse with a great fit.', ['Tops']),\n",
    "     ('The most gorgeous pair of jeans I have seen.', ['Bottoms']),\n",
    "     ('This bra is silky smooth material and fits perfectly.', ['Intimate'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-representative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rev in revs:\n",
    "    c_res = pipeline.predict(create_test_data(rev))\n",
    "    print(rev[0], '\\n', rev[1], '\\n' ,mlb.inverse_transform(c_res), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-recommendation",
   "metadata": {},
   "source": [
    "**Observations**:  \n",
    "\n",
    "- The first three sample sentences do not have a problem with the classification.  They do contain keywords that are highly realted to the Deparement names.  `Dress`, `Pants`, and `Blouse`.  \n",
    "- The fourth sentence miss-classifed with a match to both `tops` and `bottoms`.\n",
    "- The fifth sentence also miss classifed with getting one correct, but miss matching on `bottoms` and `dresses`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-slovakia",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "**Parts of Speech**  \n",
    "\n",
    "The most interesting takeaway from this portion of the exercise was the negative adjectives and adverbs.  These descriptors of the products help point us in a direction we might not have been aware of.  For example, the following top words identified above:\n",
    "\n",
    " - **Positive**: Top words are `top`, `great`, `perfect`, `really`, and `pretty`\n",
    " - **Negative**: Top words are `small`, `little`, `thin`, `tight`, and `short`\n",
    "\n",
    "The negative words suggest there appears to be a **size accuracy issue** where products are smaller or larger than people expect vs. the sizes claimed. \n",
    "\n",
    "**Sentiment Analysis**\n",
    "\n",
    " - Our model to predict recommendations based on sentiment was very accurate.  Using the `SVC` classifier tuned to the model, we achieved about a `0.915` accuracy in predictions on our test data set.  With `91.5%` confidence, we can determine how a person will rate our products based on the reviews they write.\n",
    "\n",
    "**Department Classification**\n",
    "\n",
    "- This exercise proved interesting as well.  While the overall performance (as measured by `f1 macro`) was not as strong as the sentiment-based model, it also provided different points of view we could not achieve otherwise.  In the sample selections, we can see that those were often quite relevant to the prediction in the cases where it misclassified with multiple categories/departments.  Therefore it might be possible to **cross-list/market** products in these categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-circular",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.TextBlob.tags\n",
    "1. https://stackoverflow.com/questions/55604249/featureunion-vs-columntransformer\n",
    "1. https://chrisalbon.com/python/data_wrangling/pandas_expand_cells_containing_lists/\n",
    "1. https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "1. https://sklearn.org/modules/svm.html#complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-anchor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
